---
title: System Design 102
updated: 2021-01-22 00:00
comments: true
mailchimp: true
layout: post
image: /images/system_design_holistic_cover.png
excerpt: M√°s conceptos b√°sicos sobre System Design que cualquier Software Engineer deber√≠a entender.
---

Puedes leer este post en ingl√©s [aqu√≠](/system-design-102).

Dicen que las segundas partes nunca son tan buenas como [las primeras](/system-design-101). Excepto el Batman de Nolan. Pero lo voy a intentar.

Las personas y las computadoras viven en una especie simbiosis. Y esto no es una coincidencia. En 1960 J. C. R. Licklider [escribi√≥ sobre esta simbiosis](https://groups.csail.mit.edu/medg/people/psz/Licklider.html). Despu√©s dedic√≥ el resto de su carrera a trabajar en ello y a financiar proyectos relacionados a trav√©s de ARPA. [Licklider](https://en.wikipedia.org/wiki/J._C._R._Licklider) fue originalmente psic√≥logo y el Internet existe porque anim√≥ activamente a las personas de su alrededor a construir una [Red de Computadoras Intergal√°ctica](https://www.kurzweilai.net/memorandum-for-members-and-affiliates-of-the-intergalactic-computer-network). Era un visionario.

¬øPor qu√© te estoy contando esto? Porque hacer software trata de interactuar con las computadoras y darles instrucciones. Al decirles lo que tienen que hacer, las limitaciones y los problemas surgen principalmente porque somos humanos. [Conway](https://en.wikipedia.org/wiki/Conway%27s_law) lo sab√≠a mejor que nadie.

> Las organizaciones dedicadas al dise√±o de sistemas [...] est√°n condenadas a producir dise√±os que son copias de las estructuras de comunicaci√≥n de dichas organizaciones. ‚Äî ‚ÄâMelvin E. Conway

¬°Que no te enga√±en! En alg√∫n momento de tu vida dividir√°s un monolito. Y ser√° principalmente porque eres humano y no tanto porque tenga sentido. Al hacerlo, entras en el mundo de los [sistemas distribuidos](https://es.wikipedia.org/wiki/Computaci%C3%B3n_distribuida). Y puede que al principio te de miedo, y con raz√≥n. Pero sigue leyendo.

![](/images/system_design_scared.png)

[Comparte este post en üê¶ Twitter](https://twitter.com/intent/tweet?text={{page.title}}&url={{site.url}}{{page.url}}&via={{site.twitter_username}}&related={{site.twitter_username}}) o suscr√≠bete a mi newsletter **With a grain of salt** para recibir un email con novedades cada cierto tiempo.

{% include mailchimp.html %}

## [Meiosis](https://es.wikipedia.org/wiki/Meiosis)

Hay muchas formas de dividir un monolito, pero en cualquiera de ellas eliminar√°s la complejidad de una base de c√≥digo y la a√±adir√°s al sistema. El c√≥digo que sol√≠as invocar con una funci√≥n ahora est√° esparcido en varios servicios. Y esos servicios deben comunicarse entre s√≠. Ese ser√° tu desaf√≠o.

![](/images/system_design_split_backend.png)

Si este diagrama te parece desordenado y extra√±o es porque lo es. La aplicaci√≥n frontend interact√∫a con varios servicios de backend.

## [Baldur's Gate](https://es.wikipedia.org/wiki/Baldur%27s_Gate)

Nunca he jugado a este juego, pero imagino que tiene algo que ver con encontrar el camino hacia alg√∫n tipo de puerta √∫nica. Perd√≥n. Lo que quiero decir en realidad es que, en lugar de exponer el dise√±o interno del sistema, ser√≠a muy bueno tener una sola interfaz. Dar√≠a la ilusi√≥n de interactuar con un solo servicio de backend. Esa "puerta" proporcionar√≠a una experiencia cohesiva, uniforme y unificada.

¬°Buenas noticias! Ya hay personas que han tenido este problema antes que t√∫. As√≠ que puedes usar un [proxy inverso](https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/) llamado [API Gateway](https://www.redhat.com/en/topics/api/what-does-an-api-gateway-do). Algunos de los m√°s famosos son [Kong Gateway](https://konghq.com/kong/) y [el que ofrece AWS](https://aws.amazon.com/es/api-gateway/).

![](/images/system_design_api_gateway.png)

Pero un API Gateway sirve para mucho m√°s:

1. [Autenticaci√≥n](https://konghq.com/learning-center/api-gateway/api-gateway-authentication/). En lugar de dejar que cada servicio de backend maneje la autenticaci√≥n, puedes dejar que el API Gateway se ocupe de ello.

2. Regular el trafico. Rate limiting, [throttling](https://docs.aws.amazon.com/es_en/apigateway/latest/developerguide/api-gateway-request-throttling.html) y restricci√≥n de tr√°fico. M√°s que nada porque probablemente quieras proteger tus servicios de [ataques DDoS](https://es.wikipedia.org/wiki/Ataque_de_denegaci%C3%B3n_de_servicio).

3. Analytics. Visualizar, inspeccionar y supervisar el tr√°fico de los servicios backend. Porque seguramente querr√°s entender c√≥mo se utilizan tus APIs.

4. Transformaciones. Puedes transformar tanto peticiones como respuestas sobre la marcha.

5. [Serverless](https://www.serverless.com/amazon-api-gateway). Podr√≠as invocar [funciones lambda](https://www.serverless.com/framework/docs/providers/aws/guide/intro/) como si fueran una API. No miento.


![](/images/system_design_api_gateway_explained.png)


## [Call me maybe](https://youtu.be/fWNaR-rxAic)

Una vez hayas resuelto los problemas de comunicaci√≥n entre backend y frontend, es hora de resolver los problemas entre los propios servicios backend.

Claro que puedes utilizar llamadas HTTP. Pero si est√°s monitorizando al menos las [4 Se√±ales de Oro](https://sre.google/sre-book/monitoring-distributed-systems/), eventualmente vas a observar un aumento en la latencia. Espero que esto no te pille por sorpresa. Hacer una llamada HTTP tarda m√°s tiempo que invocar una funci√≥n. Entonces no, incluso si tu c√≥digo est√° aislado, no puedes ignorar el dise√±o de todo el sistema.

No me malinterpretes. Este no es un mal dise√±o y en la mayor√≠a de los casos probablemente funcionar√° durante alg√∫n tiempo. Pero eventualmente necesitar√°s una soluci√≥n m√°s decente.

En alg√∫n momento, la latencia se convertir√° en un problema y alguien con m√°s experiencia que t√∫ en System Design te propondr√° utilizar alg√∫n tipo de comunicaci√≥n asincr√≥nica.

En el post anterior ya expliqu√© qu√© son las [colas de mensajes](https://aws.amazon.com/es/message-queue/) y m√°s espec√≠ficamente el patr√≥n [cola de tareas](https://redislabs.com/ebook/part-2-core-concepts/chapter-6-application-components-in-redis/6-4-task-queues/), tambi√©n conocido como [cola de trabajo](https://www.rabbitmq.com/tutorials/tutorial-two-python.html).

![](/images/system_design_queue_workers.png)

Un _publisher_ distribuye tareas a trav√©s de una cola de mensajes entre _workers_ que [compiten para consumirlos](https://www.enterpriseintegrationpatterns.com/patterns/messaging/CompetingConsumers.html). S√≥lo un servicio y una instancia consume el mensaje y ejecuta la tarea/trabajo.

Pero, ¬øy si no quieres que haya competici√≥n?. ¬øQu√© pasa si quieres enviar mensajes de forma asincr√≥nica a varios servicios a la vez? Resulta que [RabbitMQ](https://www.rabbitmq.com/), [Apache Kafka](https://kafka.apache.org/) y [Google Pub/Sub](https://cloud.google.com/pubsub/) permiten que un servicio _publisher_ env√≠e un mensaje a varios servicios _consumers_. Este patr√≥n se conoce como [publish/subscribe](https://www.rabbitmq.com/tutorials/tutorial-three-python.html).

![](/images/system_design_pub_sub_explained.png)

No necesitas preocuparte demasiado por los detalles, pero es bueno entender c√≥mo funciona el [patr√≥n pub/sub](https://thenewstack.io/publish-subscribe-introduction-to-scalable-messaging/) a grandes rasgos. El _publisher_, tambi√©n llamado _producer_, nunca env√≠a un mensaje directamente a una cola. De hecho, a menudo el _producer_ ni siquiera sabe si el mensaje se entregar√° a alguna cola.

En cambio, el _producer_ solo env√≠a mensajes a un _exchange_. Un _exchange_ es algo muy simple. Por un lado recibe mensajes de los _producers_ y por otro lado los env√≠a a las colas que haya. Cada servicio tiene su propia cola pero s√≥lo una instancia de cada servicio consume el mensaje.

Si alguna vez alguien menciona algo llamado [bus de eventos](https://docs.microsoft.com/es-es/dotnet/architecture/microservices/multi-container-microservice-net-applications/integration-event-based-microservice-communications), esto es de lo que est√°n hablando. O esto o algo muy parecido. Juntando todas las piezas puedes tener una visi√≥n m√°s completa del sistema. Ahora todo vuelve a tener sentido.

![](/images/system_design_holistic.png)


## [Mitosis](https://es.wikipedia.org/wiki/Mitosis)

De vuelta al frontend. Una forma de dividir un monolito frontend podr√≠a ser muy simple. Puedes dividirlo literalmente en dos y almacenar los assets est√°ticos en diferentes sitios.

![](/images/system_design_split_frontend.png)

Peeeero no todo es un camino de rosas. Administrar el estado y la autenticaci√≥n de una sola aplicaci√≥n es sencillo. Pero, ¬øqu√© pasa cuando la divides? Ser√≠a una experiencia horrible si alguien navegara de una aplicaci√≥n a otra y necesitara autenticarse de nuevo. La navegaci√≥n entre aplicaciones deber√≠a ser imperceptible.

Podr√≠as pensar que [almacenar un token JWT](https://stormpath.com/blog/where-to-store-your-jwts-cookies-vs-html5-web-storage) en _local storage_ ser√≠a una buena idea. Pero no. Stop. Caca. Mal. No hagas eso. El almacenamiento local est√° dise√±ado para ser accesible a trav√©s de javascript, por lo que no proporciona ninguna protecci√≥n contra ataques [XSS](https://en.wikipedia.org/wiki/Cross-site_scripting). Es mucho m√°s seguro utilizar una [cookie de sesi√≥n http only](https://www.whitehatsec.com/glossary/content/httponly-session-cookie).

## Div√≠de y vencer√°s. O no.

Seguro que ya has o√≠do hablar de los [microfrontends](https://martinfowler.com/articles/micro-frontends.html). Se supone que ayudan a escalar el desarrollo para que muchos equipos trabajen simult√°neamente en un producto grande y complejo. Puedes incluso [combinar microfrontends con serverless](https://blog.bitsrc.io/serverless-microfrontends-in-aws-999450ed3795). Pero no voy a entrar en detalles para que nadie me tire piedras. Ni tampoco voy a hablar de [server side rendering](https://sbstjn.com/blog/serverless-create-react-app-server-side-rendering-ssr-lambda/). Lo √∫nico que quiero es que entiendas que dividir aplicaciones frontend no afecta solo al c√≥digo. Siempre hay consecuencias de infraestructura.

![](/images/system_design_micro_frontends.png)

Dividir aplicaciones no reducir√° la complejidad. Simplemente la **distribuir√°** por todo el sistema.

## Saber y ganar

Hoy en d√≠a estamos acostumbrados a tratar con sistemas distribuidos. Pero no siempre ha sido as√≠. Perm√≠teme una breve lecci√≥n de historia. Las computadoras sol√≠an funcionar de forma aislada. Eso fue hasta que naci√≥ [ARPANET](https://en.wikipedia.org/wiki/ARPANET) en 1969 despu√©s de un parto que dur√≥ varios a√±os. El primer sistema distribuido. Mas o menos.

Uno de los primeros cuatro nodos fue la Universidad de California, Los √Ångeles. Ten√≠an parte de la infraestructura necesaria porque ya hab√≠an intentado conectar los campus de la UCLA algunos a√±os antes, pero como no se llevaban bien, fracasaron. Despu√©s de ese primer intento, la UCLA redirigi√≥ sus fuerzas a otro proyecto que ten√≠a como objetivo **medir** c√≥mo los bits se mov√≠an de un lugar a otro en un sistema inform√°tico.

[Len Kleinrock](https://en.wikipedia.org/wiki/Leonard_Kleinrock) de la UCLA, quien estuvo involucrado en el dise√±o de la ARPANET, literalmente d√≠o un pu√±etazo en la mesa y exigi√≥ **medir** todo lo que ocurr√≠a en la red. Su argumento fue que ARPANET era un experimento. Y como tal, hab√≠a que medirlo todo: cu√°ntos paquetes viajaban por la red, d√≥nde se estaban formando los cuellos de botella, qu√© velocidad y capacidad se pod√≠an lograr en la pr√°ctica y qu√© condiciones de tr√°fico romper√≠an la red.

Si te das cuenta, todo eso es muy similar a las [4 Se√±ales de Oro](https://sre.google/sre-book/monitoring-distributed-systems/) que se atribuyen a Google hoy en d√≠a. Nada mas lejos de la verdad pero nos gusta reinventar la rueda cada pocos a√±os.

Las personas de la UCLA estaban interesadas en sacarle provecho a su proyecto, eso es verdad. ¬øPero a qu√© m√°s da? Gracias a los cimientos de un proyecto fallido y el impulso del proyecto de **monitorizaci√≥n**, la UCLA se convirti√≥ en el **Centro de Medici√≥n de Redes** de la ARPANET. Esto fue en 1969. Justo cuando los Rolling Stones eran considerados la banda de rock m√°s grande del mundo. Si hace 50 a√±os, s√≠, hace 50 a√±os, las personas se preocupaban por la monitorizaci√≥n, ¬øpor qu√© no lo har√≠as t√∫ hoy?

## M√≠rame

No estoy aqu√≠ para decirte qu√© monitorizar. Ese es un tema para un post futuro. Por ahora solo quiero explicarte c√≥mo suelen funcionar las herramientas de monitorizaci√≥n a nivel muy b√°sico.

Las herramientas m√°s tradicionales, basadas en _push_, env√≠an m√©tricas al servidor de m√©tricas, tambi√©n conocido como _Aggregator_. Por lo general, tienes que instalar un agente junto a cada servicio que quieres monitorizar. A veces en el mismo contendor; otras veces en un [contenedor sidecar](https://medium.com/bb-tutorials-and-thoughts/kubernetes-learn-sidecar-container-pattern-6d8c21f873d). Tienes que configurar cu√°ndo, d√≥nde y con qu√© frecuencia enviar las m√©tricas. Pero enviar m√©tricas no es el objetivo principal de tus servicios, por lo que en lugar de enviarlas una por una, tambi√©n tienes que decidir si las m√©tricas deben agregarse y c√≥mo se deben agregar antes de enviarlas. Aunque, si usas algo como [New Relic](https://newrelic.com/resource/key-capabilities-new-relic-apm), no tienes que preocuparse demasiado por los detalles de implementaci√≥n.

![](/images/system_design_monitoring_push.png)

Pero puedes dise√±ar el sistema de monitorizaci√≥n de una manera completamente transparente para tus servicios. Las herramientas basadas en _pull_ en vez de _push_ como [Prometheus](https://danielfm.me/prometheus-for-developers/) lo hacen posible. Debido a que la configuraci√≥n no est√° vinculada a los servicios, ya no tendr√°s que re-desplegar tus servicios si quieres cambiarla.

![](/images/system_design_monitoring_pull.png)

As√≠ es como podr√≠a funcionar una herramienta de monitorizaci√≥n basada en _pull_:

1. Todos tus servicios deben tener un endpoint `/metrics`. Esto es super com√∫n y puedes encontrar muchas librer√≠as y/o frameworks que te lo dan gratis.

2. El _Aggregator_ necesita saber d√≥nde est√°n los servicios a los que pedir las m√©tricas. Puedes definirlos de forma est√°tica pero ¬øqu√© pasar√≠a si una IP cambia? ¬øO si un servicio se cae? Quiz√° ser√≠a mejor utilizar alg√∫n tipo de [Service Discovery](https://www.nginx.com/blog/service-discovery-in-a-microservices-architecture/) que le diga autom√°ticamente al _Aggregator_ de d√≥nde extraer las m√©tricas. Quiz√° puedas usar [Consul](https://www.consul.io/use-cases/service-discovery-and-health-checking) para eso.

3. El _Aggregator_ env√≠a peticiones HTTP al endpoint `/metrics` de forma regular. Prometheus llama a eso un _scrape_. La respuesta se analiza y se guarda en el almacenamiento local.

4. Algunos servicios, por la raz√≥n que sea, son m√°s dif√≠ciles de instrumentar. En esos casos podr√≠as desplegar un _[Exporter](https://prometheus.io/docs/instrumenting/exporters/)_ al lado de ese servicio que proporcionar√° el endpoint `/metrics`.

5. Imagino que querr√°s visualizar y analizar las m√©tricas de alguna manera. Hay muchas herramientas que te permiten hacer eso pero [Grafana](https://grafana.com/) es la m√°s famosa. Te permite consultar, visualizar y comprender tus m√©tricas.

6. Algunos agregadores de m√©tricas como Prometheus tambi√©n se pueden configurar para enviar alertas cuando ocurre algo inesperado. Un _[Alert Manager](https://prometheus.io/docs/alerting/latest/alertmanager/)_ puede recibir alertas del servidor de m√©tricas y convertirlas en notificaciones.

## L√©eme

El [logging](https://medium.com/redbox-techblog/building-an-open-data-platform-logging-with-fluentd-and-elasticsearch-4582de868398) es otro mecanismo √∫til que te da visibilidad sobre el comportamiento de un servicio. Tradicionalmente, era com√∫n escribir los registros en un archivo llamado _logfile_. Pero hoy en d√≠a un servicio [no deber√≠a preocuparse con guardar logs en disco](https://12factor.net/logs). Deber√≠a escupir los logs a la salida est√°ndar o _stdout_. Hay muchos beneficios de hacer eso. Durante el desarrollo local, puedes ver los logs en tu terminal.

![](/images/system_design_logging.png)

Mientras que en un entorno de producci√≥n, los logs se enviar√°n a un procesador de logs como [Fluend](https://medium.com/redbox-techblog/building-an-open-data-platform-logging-with-fluentd-and-elasticsearch-4582de868398). All√≠ se filtrar√°n y luego se almacenar√°n en una base de datos como [Elasticsearch](https://www.elastic.co/what-is/elasticsearch). Al igual que con las m√©tricas, querr√°s buscar, visualizar y analizar los registros con una herramienta como [Kibana](https://www.elastic.co/what-is/kibana).

## ¬øHemos llegado ya?

S√© que hemos hablado de muchos conceptos y herramientas. Y esta √∫ltima parte sobre las m√©tricas y el logging no es f√°cil de comprender. No te preocupes. Lee el post de nuevo e indaga en los recursos que te he dejado entre las l√≠neas. 

No hemos terminado. No exactamente. Los sistemas distribuidos manejan datos distribuidos. Y ese ser√° el tema del pr√≥ximo post de esta serie. Y eso que todav√≠a no hemos hablado de Kubernetes.

[Comparte este post en üê¶ Twitter](https://twitter.com/intent/tweet?text={{page.title}}&url={{site.url}}{{page.url}}&via={{site.twitter_username}}&related={{site.twitter_username}}) o suscr√≠bete a mi newsletter **With a grain of salt** para recibir un email con novedades cada cierto tiempo.
