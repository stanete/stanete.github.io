---
title: Crear un filtro de spam con Machine Learning
updated: 2017-01-23 08:45
comments: true
mailchimp: true
image: /images/filtro-spam-machine-learning.png
---

> Extraer caracter칤sticas, tokenizaci칩n, & clasificaci칩n de documentos

Este es el tercer post de **Machine Learning pero sin hype**, una serie sobre aprendizaje autom치tico (o de m치quinas) en espa침ol.

Crear un filtro de spam es el **"Hola Mundo"** de la clasificaci칩n de documentos con Machine Learning. Por eso, en este post vamos a crear un filtro de spam bastante preciso a partir de emails reales etiquetados como *spam* o *ham* (emails que no son spam).

Necesitamos tener instalado [python 3.5 o 3.6](https://www.python.org/) y varios paquetes:

- [numpy](http://www.numpy.org/)
- [scipy](https://www.scipy.org/)
- [pandas](http://pandas.pydata.org/)
- [scikit-learn](http://scikit-learn.org/)

Si los instalamos en este orden via *pip*, no deber칤amos tener ning칰n problema. Yo utilizo [PyCharm](https://www.jetbrains.com/pycharm/) y tengo todo dentro de un [virtualenv](https://virtualenv.pypa.io/en/stable/).

El 칰nico paquete con el que no hemos trabajado todav칤a es **pandas**. De momento s칩lo necesitamos saber que es una librer칤a de python opensource para an치lisis de datos.

El proceso que seguir칠mos ser치 el mismo que en el [primer post](introduccion-machine-learning-sin-hype):

1. Recolectar y preparar datos
2. Escoger un modelo
3. Entrenar el modelo con los datos
4. Probar el modelo

<div class="divider"></div>

## Recolectar y preparar datos

Vamos a tener que enamorarnos de este paso porque es en el que vamos a pasar gran parte del tiempo a la hora de trabajar en Machine Learning.

### Descargar datos

Vamos a utilizar el dataset p칰blico [Enron-Spam](http://www.aueb.gr/users/ion/data/enron-spam/) que vamos a descargar y poner en una carpeta llamada *data*. Este conjunto de datos contiene emails preprocesados divididos en carpetas *spam* y *ham*.

Primero tenemos que definir las etiquetas (o clases) de este dataset y las rutas de las carpetas que contienen los emails que hemos descargado.

```python
# Etiquetas.
HAM = 'ham'
SPAM = 'spam'

# Fuentes de datos con las rutas de
# las carpetas que contienen los emails
# y las etiquetas correspondientes.
SOURCES = [
    ('data/enron1/spam', SPAM),
    ('data/enron1/ham', HAM),
    ('data/enron2/spam', SPAM),
    ('data/enron2/ham', HAM),
    ('data/enron3/spam', SPAM),
    ('data/enron3/ham', HAM)
]
```

### Cargar datos

Vamos a cargar los datos en un [DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html), una estructura de datos en la que podemos pensar como un diccionario construido sobre **numpy** (eso quiere decir que es r치pido). El DataFrame contendr치 los cuerpos de los emails en una columna y la etiqueta correspondiente en otra.

```python
from pandas import DataFrame

# Definir dos columnas. Una para el texto de los
# emails y otra para la etiqueta correspondiente.
data = DataFrame({'text': [], 'label': []})
```

Vamos a crear una funci칩n que, dada la ruta de una carpeta, va a leer todos los ficheros de esa carpeta y va a devolver la ruta del fichero junto con el texto del email. Vamos a utilizar la ruta del fichero como index para cada entrada del DataFrame.

```python
import os

def read_files(path):
    # Obtener los nombres de todos los ficheros.
    file_names = os.listdir(path)

    # Recorrer los ficheros.
    for file_name in file_names:

        # La ruta del fichero.
        file_path = os.path.join(path, file_name)

        lines = []

        # Abrir el fichero.
        f = open(file_path, encoding="latin-1")
        for line in f:
            # A침adir cada l칤nea del email.
            lines.append(line)

        # Cerrar el fichero.
        f.close()

        # Unir las l칤neas.
        text = '\n'.join(lines)

        # Devolver la ruta del fichero y el texto.
        yield file_path, text
```

Tambi칠n vamos a definir una funci칩n que va a construir y devolver un DataFrame a partir de los cuerpos de los emails obtenidos con *read_files*. Esta funci칩n tambi칠n va a asignarle a cada email con su etiqueta (label) correspondiente. Cada entrada del DataFrame va a ser indexada por la ruta del archivo de cada email.

```python
def build_data_frame(path, label):
    rows = []
    index = []

    # Recorrer todos los ficheros dada un ruta.
    for file_path, text in read_files(path):
        # A침adir el texto del fichero junto
        # con su etiqueta correspondiente.
        rows.append({'text': text, 'label': label})

        # Utilizar la ruta del fichero como index.
        index.append(file_path)

    # Crear un nuevo DataFrame
    # a partir de las filas y los indexes.
    data_frame = DataFrame(rows, index=index)

    # Devolver el DataFrame.
    return data_frame
```

Vamos a usar estas dos funciones, *read_files* y *build_data_frame*, para construir un DataFrame a partir de los emails que tenemos:

```python
# Recorrer las rutas de las
# carpetas que contienen los emails:
for path, label in SOURCES:
    # A침adir cada DataFrame que devuelve
    # la funci칩n build_data_frame dada la
    # ruta de la carpeta y su etiqueta correspondiente.
    data = data.append(build_data_frame(path, label))
```

Para asegurarnos de que los datos no est치n ordenados de forma uniforme, vamos a mezclarlos:

```python
import numpy

data = data.reindex(numpy.random.permutation(data.index))
```

### Extraer caracter칤sticas

Antes de que podamos entrenar un algoritmo para clasificar los emails, necesitamos unas **caracter칤sticas** (features). En la clasificaci칩n de documentos, la frecuencia con la que aparece cada palabra es una buena caracter칤stica.

Vamos a crear una tabla con todas las palabras mencionadas en el **corpus**(colleci칩n de emails que tenemos) y su frecuencia en cada clase de email (spam o ham):

|      |    Linux   |   today    |   Viagra   |    Free    |
|:----:|:----------:|:----------:|:----------:|:----------:|
| ham  | 619        | 67         | 0          | 50         |
| spam | 3          | 432        | 291        | 534        |

Este modelo o proceso se llama **[bolas de palabras](https://es.wikipedia.org/wiki/Modelo_bolsa_de_palabras)** o **tokenizaci칩n** porque **transformamos una colecci칩n de documentos de texto a una matriz de recuento de tokens**. Con *scikit-learn* esto es muy f치cil:


```python
from sklearn.feature_extraction.text import CountVectorizer

count_vectorizer = CountVectorizer()

# Los emails que tenemos en el DataFrame.
email_bodies = data['text'].values

# Aprender el vocabulario del corpus
# y extraer el recuento de tokens.
features = count_vectorizer.fit_transform(email_bodies)

# Las etiquetas (spam o ham).
labels = data['label'].values
```

### Datos de entrenamiento y datos de testeo

Por 칰ltimo, necesitamos **dividir** este dataset en datos para entrenar el modelo y datos para testearlo.

```python
from sklearn.model_selection import train_test_split

features_train, features_test, \
    labels_train, labels_test = train_test_split(
        features, labels,
        train_size=0.8,
        test_size=0.2,
        random_state=0)
```

<div class="divider"></div>

## Escoger un modelo

El problema que estamos intentando resolver es de **aprendizaje supervisado**, concretamente, **clasificaci칩n**.

Vamos a usar un [**clasificador bayesiano ingenuo**](clasificador-bayesiano-ingenuo), uno de los clasificadores m치s sencillos y r치pidos. Este tipo de clasificador es incre칤blemente efectivo en la detecci칩n de spam y clasificaci칩n de documentos en general.

```python
from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB()
```

<div class="divider"></div>

## Entrenar el modelo con los datos

Vamos a **entrenar el clasificador** con los datos que hemos dividio:

```python
clf.fit(features_train, labels_train)
```

Este es el **proceso de aprendizaje**. A partir de ejemplos etiquetados, el clasificador bayesiano **aprende** las probabilidades que cada token aporta a esa etiqueta (o clase). Dado un nuevo email, el clasificador usar치 esas probabilidades para predecir su etiqueta (o clase), en este caso *spam* o *ham*.

<div class="divider"></div>

## Probar el modelo

Ya hemos entrenado el clasificador pero no sabemos c칩mo de **preciso** es a la hora de clasificar nuevos emails. Para averiguarlo vamos a testear el clasificador con los datos que hemos dividido antes. Vamos a hacer que nuestro clasificador determine (o prediga) las etiquetas de algunas emails y compararlas con las etiquetas reales.

```python
from sklearn.metrics import accuracy_score

# Predecir etiquetas para los emails de testeo.
labels_predict = clf.predict(features_test)

# Calcular la precisi칩n comparando
# las etiquetas predecidas y las etiquedas reales.
accuracy = accuracy_score(labels_predict, labels_test)

print("Accuracy: %.2f" % (accuracy))

# Accuracy: 0.99
```

Wow! Una precisi칩n del 99% 游땹

Tambi칠n podemos probar nuestro filtro de spam con nuevos emails:

```python
# Emails de prueba.
test_emails = ['Free Viagra!',
               "I'm going to attend the meeting tomorrow."]

# Tokenizaci칩n.
test_features = count_vectorizer.transform(test_emails)

# Predecir etiquetas.
labels_predict = clf.predict(example_features)

print("Predictions: %s" % labels_predict)

# Predictions: ['spam' 'ham']
```

El filtro de spam funciona a la perfecci칩n 游꿀游꿀游꿀!!

<div class="divider"></div>

## Vamos a terminar esto

Los clasificadores bayesianos ingenuos o Na칦ve Bayes funcionan muy bien para clasificar documentos. Y aunque en la vida real los filtros de spam son mucho m치s complejos, el nuestro funciona tambi칠n bastante bien.

<div class="divider"></div>

Te puedes suscribir y recibir un email cada vez que haya un nuevo post.
